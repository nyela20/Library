{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5ccd44-8f66-4eb9-95cd-54659ec43aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aperçu ===\n",
      "          Type de document  Prêts 2022  \\\n",
      "0  Bande dessinée jeunesse         814   \n",
      "1  Bande dessinée jeunesse         602   \n",
      "2  Bande dessinée jeunesse         989   \n",
      "3  Bande dessinée jeunesse         688   \n",
      "4    Bande dessinée adulte         624   \n",
      "\n",
      "                                              Titre  \\\n",
      "0                    Game over. 6. Sound of silence   \n",
      "1                                        Note to be   \n",
      "2         Max et Lili ont peur des images violentes   \n",
      "3  La Rose écarlate : missions. 3. La dame en rouge   \n",
      "4                            Les cigares du pharaon   \n",
      "\n",
      "                      Auteur  Nombre de localisations  Nombre de prêt total  \\\n",
      "0                        NaN                       41                  3559   \n",
      "1                      Erroc                       36                  2262   \n",
      "2  Saint-Mars,  Dominique de                       48                  5408   \n",
      "3         Lyfoung,  Patricia                       46                  2436   \n",
      "4                      Hergé                       55                  3760   \n",
      "\n",
      "   Nombre d'exemplaires  \n",
      "0                    53  \n",
      "1                    49  \n",
      "2                    97  \n",
      "3                    55  \n",
      "4                    69  \n",
      "\n",
      "=== Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 842 entries, 0 to 841\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Type de document         842 non-null    object\n",
      " 1   Prêts 2022               842 non-null    int64 \n",
      " 2   Titre                    842 non-null    object\n",
      " 3   Auteur                   827 non-null    object\n",
      " 4   Nombre de localisations  842 non-null    int64 \n",
      " 5   Nombre de prêt total     842 non-null    int64 \n",
      " 6   Nombre d'exemplaires     842 non-null    int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 46.2+ KB\n",
      "None\n",
      "\n",
      "=== Valeurs manquantes ===\n",
      "Type de document            0\n",
      "Prêts 2022                  0\n",
      "Titre                       0\n",
      "Auteur                     15\n",
      "Nombre de localisations     0\n",
      "Nombre de prêt total        0\n",
      "Nombre d'exemplaires        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib # Importez joblib\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"bookParis.csv\", \n",
    "    sep=';',                \n",
    "    encoding='utf-8',        # pour gérer les accents\n",
    "    on_bad_lines='skip'      # ignore les lignes mal formées\n",
    ")\n",
    "\n",
    "# Pour Voir les premières lignes de dataset\n",
    "print(\"=== Aperçu ===\")\n",
    "print(df.head())\n",
    "\n",
    "# Vérifier les infos et types\n",
    "print(\"\\n=== Info ===\")\n",
    "print(df.info())\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "print(\"\\n=== Valeurs manquantes ===\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d743cc-9e03-4d63-a1ab-e5c08be0bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (0.120.0)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (0.38.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\rimha\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (1.5.2)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from fastapi) (0.48.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from fastapi) (2.12.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from fastapi) (4.15.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from fastapi) (0.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn) (8.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rimha\\appdata\\roaming\\python\\python313\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn nest_asyncio joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d4e2cf-0817-4103-be7f-7b213fd8156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(\"bookParis.csv\", sep=';')\n",
    "\n",
    "# Colonnes catégorielles à encoder que on doit conventir\n",
    "categorical_cols = ['Type de document', 'Auteur']\n",
    "\n",
    "# Remplacer les NaN par 'Inconnu' pour l'encodage\n",
    "df['Auteur'] = df['Auteur'].fillna('Inconnu')\n",
    "\n",
    "# Encoder les colonnes\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e13bc9-5906-4d0e-abd0-d68be803d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Nombre de localisations\", \"Nombre d'exemplaires\", \"Type de document_encoded\", \"Auteur_encoded\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b7a2fb-9795-4a73-b7d1-68bfb7450ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Titre  Prêts 2022  \\\n",
      "217                       Ça sent la croquette !        1660   \n",
      "617                               Prout atomique        1607   \n",
      "420                    A la pêche aux nouilles !        1602   \n",
      "649                              Karmastrophique        1617   \n",
      "716                               Funky moumoute        1583   \n",
      "305                            Big bisous baveux        1582   \n",
      "462                               Jurassic mamie        1540   \n",
      "733  Mortelle Adèle. 2. L'enfer c'est les autres        1563   \n",
      "707                           Tout ça finira mal        1561   \n",
      "31        Mortelle Adèle.  T.10 .  Choubidoulove        1515   \n",
      "\n",
      "     Predicted_Prêts_2022  \n",
      "217               1631.57  \n",
      "617               1609.25  \n",
      "420               1594.16  \n",
      "649               1588.63  \n",
      "716               1570.64  \n",
      "305               1561.12  \n",
      "462               1542.08  \n",
      "733               1534.02  \n",
      "707               1520.04  \n",
      "31                1510.57  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "target = 'Prêts 2022'\n",
    "\n",
    "# phase d'entainement de modèle \n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(df[features], df[target])\n",
    "\n",
    "# Faire les prédictions et ajouter la colonne\n",
    "df['Predicted_Prêts_2022'] = model.predict(df[features])\n",
    "\n",
    "# Afficher les top 10 livres prédits les plus empruntés\n",
    "top_predicted = df.sort_values(by='Predicted_Prêts_2022', ascending=False).head(10)\n",
    "print(top_predicted[['Titre', 'Prêts 2022', 'Predicted_Prêts_2022']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b03822-8d9b-4d1c-afdc-0a869f0885ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE du modèle : 84.25\n",
      "                                           Titre  Prêts 2022  \\\n",
      "617                               Prout atomique        1607   \n",
      "217                       Ça sent la croquette !        1660   \n",
      "420                    A la pêche aux nouilles !        1602   \n",
      "649                              Karmastrophique        1617   \n",
      "305                            Big bisous baveux        1582   \n",
      "462                               Jurassic mamie        1540   \n",
      "716                               Funky moumoute        1583   \n",
      "31        Mortelle Adèle.  T.10 .  Choubidoulove        1515   \n",
      "733  Mortelle Adèle. 2. L'enfer c'est les autres        1563   \n",
      "707                           Tout ça finira mal        1561   \n",
      "\n",
      "     Predicted_Prêts_2022  \n",
      "617               1619.58  \n",
      "217               1619.58  \n",
      "420               1583.34  \n",
      "649               1577.03  \n",
      "305               1568.22  \n",
      "462               1546.60  \n",
      "716               1544.62  \n",
      "31                1537.85  \n",
      "733               1523.73  \n",
      "707               1520.21  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Séparer en train et test (80% train/20 test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Création et entraîner le modèle\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur le test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "print(f\"RMSE du modèle : {rmse:.2f}\")\n",
    "\n",
    "# Ajoute de la prédiction à tout le dataset\n",
    "df['Predicted_Prêts_2022'] = model.predict(df[features])\n",
    "\n",
    "# Top 10 livres prédits les plus empruntés\n",
    "top_predicted = df.sort_values(by='Predicted_Prêts_2022', ascending=False).head(10)\n",
    "print(top_predicted[['Titre', 'Prêts 2022', 'Predicted_Prêts_2022']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155bf7f0-773c-4355-b4b6-688f5b11ca6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c97f12-57f6-4f84-881f-b562330b454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Train : 0.97\n",
      "R² Test  : 0.84\n"
     ]
    }
   ],
   "source": [
    "# Score R² sur l'ensemble d'entraînement\n",
    "r2_train = model.score(X_train, y_train)\n",
    "# Score R² sur l'ensemble de test\n",
    "r2_test = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"R² Train : {r2_train:.2f}\")\n",
    "print(f\"R² Test  : {r2_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13c4546-edad-4e42-919d-3fb1989fd8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Train : 0.9342073552423711\n",
      "R² Test : 0.8554330949580757\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'squared'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR² Train :\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2_score(y_train, y_train_pred))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR² Test :\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2_score(y_test, y_test_pred))\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE Train :\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_squared_error(y_train, y_train_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE Test :\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_squared_error(y_test, y_test_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#Visualiser les prédictions vs les vraies valeurs\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#Cela te montrera si les erreurs sont réparties de façon homogène ou si ton modèle se trompe sur certains types de livres.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:196\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m params \u001b[38;5;241m=\u001b[39m func_sig\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\inspect.py:3295\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3292\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3293\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3294\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bind(args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\inspect.py:3284\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3274\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3275\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot some positional-only arguments passed as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   3276\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3281\u001b[0m             ),\n\u001b[0;32m   3282\u001b[0m         )\n\u001b[0;32m   3283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3284\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3285\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   3286\u001b[0m                 arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[0;32m   3288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[1;31mTypeError\u001b[0m: got an unexpected keyword argument 'squared'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Meilleurs paramètres trouvés\n",
    "best_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Évaluation\n",
    "print(\"R² Train :\", r2_score(y_train, y_train_pred))\n",
    "print(\"R² Test :\", r2_score(y_test, y_test_pred))\n",
    "print(\"RMSE Train :\", mean_squared_error(y_train, y_train_pred, squared=False))\n",
    "print(\"RMSE Test :\", mean_squared_error(y_test, y_test_pred, squared=False))\n",
    "\n",
    "\n",
    "\n",
    "#Visualiser les prédictions vs les vraies valeurs\n",
    "#Cela te montrera si les erreurs sont réparties de façon homogène ou si ton modèle se trompe sur certains types de livres.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Vraies valeurs (y_test)\")\n",
    "plt.ylabel(\"Prédictions\")\n",
    "plt.title(\"Comparaison des valeurs réelles vs prédictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00970f58-a811-4e38-b2dd-59d64fad9ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La nouvelle cible est 'Prêts S2 2022'. La nouvelle feature est 'Prêts S1 2022'.\n",
      "\n",
      "=== Évaluation pour la Prédiction du Second Semestre (S2) 2022 ===\n",
      "R² Test (S2)  : 0.682\n",
      "RMSE Test (S2) : 68.76\n",
      "\n",
      "=== Top 10 Livres Prédits les Plus Empruntés au Second Semestre 2022 ===\n",
      "                                           Titre  Prêts S1 2022  \\\n",
      "342                             Toi, je te zut !            646   \n",
      "433                       La rentrée des claques            611   \n",
      "225      Mortelle Adèle. 3. C'est pas ma faute !            679   \n",
      "312                    La fille de Vercingétorix            536   \n",
      "353                          Le papyrus de César            581   \n",
      "649                              Karmastrophique            807   \n",
      "733  Mortelle Adèle. 2. L'enfer c'est les autres            788   \n",
      "462                               Jurassic mamie            845   \n",
      "404                  Astérix et la Transitalique            621   \n",
      "617                               Prout atomique            731   \n",
      "\n",
      "     Prêts S2 2022  Predicted_Prêts_S2_2022  \n",
      "342            841               800.585340  \n",
      "433            844               798.935554  \n",
      "225            836               781.245481  \n",
      "312            789               777.761839  \n",
      "353            763               765.743721  \n",
      "649            810               764.147442  \n",
      "733            775               764.132033  \n",
      "462            695               731.965619  \n",
      "404            718               725.645920  \n",
      "617            876               723.514688  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Le modèle le plus performant\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CHARGEMENT ET CRÉATION DES DONNÉES PÉRIODIQUES \n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"bookParis.csv\", sep=';', encoding='utf-8', on_bad_lines='skip')\n",
    "except FileNotFoundError:\n",
    "    print(\"ATTENTION: Le fichier 'bookParis.csv' n'a pas été trouvé. Utilisation d'un chemin par défaut.\")\n",
    "    df = pd.read_csv(\"C:/Users/rimha/Downloads/bookParis.csv\", sep=';', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Création des colonnes de prêts semestriels pour l'exemple\n",
    "np.random.seed(42) # Pour la reproductibilité\n",
    "df['Prêts S1 2022'] = (df['Prêts 2022'] * np.random.uniform(0.4, 0.6, size=len(df))).astype(int)\n",
    "df['Prêts S2 2022'] = df['Prêts 2022'] - df['Prêts S1 2022']\n",
    "\n",
    "print(f\"La nouvelle cible est 'Prêts S2 2022'. La nouvelle feature est 'Prêts S1 2022'.\")\n",
    "\n",
    "\n",
    "# PRÉ-TRAITEMENT ET ENCODAGE \n",
    "\n",
    "# Gestion des valeurs manquantes\n",
    "df['Auteur'] = df['Auteur'].fillna('Inconnu')\n",
    "\n",
    "# Encodage des variables catégorielles \n",
    "categorical_cols = ['Type de document', 'Auteur']\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le \n",
    "\n",
    "# Définir les features qui sont les variables d'entrée et la CIBLE\n",
    "features = [\n",
    "    \"Prêts S1 2022\",              \n",
    "    \"Nombre de localisations\", \n",
    "    \"Nombre d'exemplaires\", \n",
    "    \"Type de document_encoded\", \n",
    "    \"Auteur_encoded\"\n",
    "]\n",
    "target = 'Prêts S2 2022' # var CIBLE\n",
    "\n",
    "#  SePARATION TRAIN et de TEST \n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# ENTRAÎNEMENT ET ÉVALUATION DU MODÈLE\n",
    "\n",
    "model_s2 = GradientBoostingRegressor(\n",
    "    n_estimators=200, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_s2.fit(X_train, y_train)\n",
    "y_test_pred = model_s2.predict(X_test)\n",
    "\n",
    "# Calcul des métriques\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test) # Calcul correct de la RMSE\n",
    "\n",
    "print(\"\\n=== Évaluation pour la Prédiction du Second Semestre (S2) 2022 ===\")\n",
    "print(f\"R² Test (S2)  : {r2_test:.3f}\")\n",
    "print(f\"RMSE Test (S2) : {rmse_test:.2f}\")\n",
    "\n",
    "# APPLICATION ET ANALYSE \n",
    "\n",
    "# Application de la prédiction à l'ensemble du DataFrame\n",
    "df['Predicted_Prêts_S2_2022'] = model_s2.predict(X)\n",
    "\n",
    "# Affichage des 10 livres prédits comme les plus empruntés au S2\n",
    "top_s2_predicted = df.sort_values(by='Predicted_Prêts_S2_2022', ascending=False).head(10)\n",
    "\n",
    "print(\"\\n=== Top 10 Livres Prédits les Plus Empruntés au Second Semestre 2022 ===\")\n",
    "print(top_s2_predicted[[\n",
    "    'Titre', \n",
    "    'Prêts S1 2022', \n",
    "    'Prêts S2 2022', \n",
    "    'Predicted_Prêts_S2_2022'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0492bcf8-c4c4-4a50-9802-6837a38aff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles regénérés avec la version actuelle de scikit-learn.\n"
     ]
    }
   ],
   "source": [
    "# Partie de API FASTAPI PYTHON\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.preprocessing import LabelEncoder       \n",
    "\n",
    "joblib.dump(model, 'ml_model.pkl')\n",
    "joblib.dump(le_dict, 'label_encoders.pkl')\n",
    "\n",
    "print(\"Modèles regénérés avec la version actuelle de scikit-learn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f46d4f8-28cd-4030-a280-0f9927964d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rimha\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb938095-c929-43d3-baa8-d9ea261c4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier de travail du Notebook est :\n",
      "C:\\Users\\rimha\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Le dossier de travail du Notebook est :\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73a8063-c4de-4570-b94c-47a60ea106f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde terminée. Le fichier top_books.pkl est créé et prêt pour l'API.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# 1. Définir les features originales\n",
    "features_original = [\n",
    "    \"Nombre de localisations\", \n",
    "    \"Nombre d'exemplaires\", \n",
    "    \"Type de document_encoded\", \n",
    "    \"Auteur_encoded\"\n",
    "]\n",
    "\n",
    "# 2. Recharger les éléments nécessaires pour garantir l'état correct\n",
    "try:\n",
    "   \n",
    "    df = pd.read_csv(\"bookParis.csv\", sep=';', encoding='utf-8', on_bad_lines='skip')\n",
    "    model = joblib.load('ml_model.pkl')\n",
    "    le_dict = joblib.load('label_encoders.pkl')\n",
    "except FileNotFoundError:\n",
    "    print(\"Erreur: Le fichier CSV ou les fichiers .pkl sont introuvables. Vérifiez les chemins.\")\n",
    "    # Sortir de la cellule ou lever une exception si les fichiers sont critiques\n",
    "    raise\n",
    "\n",
    "# 3. Reproduire le pré-traitement initial sur le DataFrame frais\n",
    "df['Auteur'] = df['Auteur'].fillna('Inconnu')\n",
    "df['Type de document_encoded'] = le_dict['Type de document'].transform(df['Type de document'])\n",
    "df['Auteur_encoded'] = le_dict['Auteur'].transform(df['Auteur'])\n",
    "\n",
    "# 4. Faire la prédiction en utilisant UNIQUEMENT les features originales\n",
    "df['Predicted_Prêts_2022'] = model.predict(df[features_original])\n",
    "\n",
    "# 5. Sélectionner et sauvegarder les top 10\n",
    "top_predicted = df.sort_values(by='Predicted_Prêts_2022', ascending=False).head(10)\n",
    "\n",
    "# 6. Préparation et sauvegarde pour l'API\n",
    "top_books_data = top_predicted[['Titre', 'Auteur', 'Prêts 2022', 'Predicted_Prêts_2022']].to_dict('records')\n",
    "joblib.dump(top_books_data, 'top_books.pkl')\n",
    "\n",
    "print(\"Sauvegarde terminée. Le fichier top_books.pkl est créé et prêt pour l'API.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e04d1bf2-1fef-4e8c-8742-8009d702b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative de chargement du fichier : C:/Users/rimha/OneDrive/Desktop/top_books.pkl\n",
      "\n",
      "--- ✅ VÉRIFICATION RÉUSSIE ---\n",
      "Type de donnée chargée : <class 'list'>\n",
      "Nombre de livres trouvés : 10\n",
      "\n",
      "--- Contenu (3 premières entrées) : ---\n",
      "Livre #1:\n",
      "  Titre: Prout atomique\n",
      "  Auteur: Mr Tan\n",
      "  Prêts Prédits: 1619.58\n",
      "--------------------\n",
      "Livre #2:\n",
      "  Titre: Ça sent la croquette !\n",
      "  Auteur: Mr Tan\n",
      "  Prêts Prédits: 1619.58\n",
      "--------------------\n",
      "Livre #3:\n",
      "  Titre: A la pêche aux nouilles !\n",
      "  Auteur: Mr Tan\n",
      "  Prêts Prédits: 1583.34\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "FILE_PATH = \"C:/Users/rimha/OneDrive/Desktop/top_books.pkl\" \n",
    "\n",
    "print(f\"Tentative de chargement du fichier : {FILE_PATH}\")\n",
    "\n",
    "try:\n",
    "    # 1. Charger le contenu\n",
    "    top_books_data = joblib.load(FILE_PATH)\n",
    "    \n",
    "    print(\"\\n--- ✅ VÉRIFICATION RÉUSSIE ---\")\n",
    "    \n",
    "    # 2. Afficher la structure (doit être une liste)\n",
    "    print(f\"Type de donnée chargée : {type(top_books_data)}\")\n",
    "    print(f\"Nombre de livres trouvés : {len(top_books_data)}\")\n",
    "    \n",
    "    # 3. Afficher les 3 premières entrées pour vérifier le contenu\n",
    "    print(\"\\n--- Contenu (3 premières entrées) : ---\")\n",
    "    for i, book in enumerate(top_books_data[:3]):\n",
    "        # Afficher uniquement les champs pertinents pour vérification\n",
    "        print(f\"Livre #{i+1}:\")\n",
    "        print(f\"  Titre: {book.get('Titre', 'N/A')}\")\n",
    "        print(f\"  Auteur: {book.get('Auteur', 'N/A')}\")\n",
    "        print(f\"  Prêts Prédits: {book.get('Predicted_Prêts_2022', 'N/A')}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n--- ❌ ERREUR : Le fichier n'a pas été trouvé à l'emplacement : {FILE_PATH} ---\")\n",
    "    print(\"Vérifiez le chemin.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n---  ERREUR LORS DU CHARGEMENT ---\")\n",
    "    print(f\"Détail : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8cfc1f4-0141-4f3e-a00d-8e195f701c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier de travail du Notebook est :\n",
      "C:\\Users\\rimha\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Le dossier de travail du Notebook est :\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc31f5-5c58-4ecc-b4bc-648e371ae2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
